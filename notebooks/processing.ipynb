{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re, string\n",
    "import json\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '38',\n",
       " 'text': 'In the shop, these MacBooks are encased in a soft rubber enclosure - so you will never know about the razor edge until you buy it, get it home, break the seal and use it (very clever con).',\n",
       " 'aspects': [{'term': 'rubber enclosure',\n",
       "   'from': 50,\n",
       "   'to': 66,\n",
       "   'polarity': 'positive'},\n",
       "  {'term': 'edge', 'from': 108, 'to': 112, 'polarity': 'negative'}]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open('../data/train_laptop_raw.json'))\n",
    "sample = data[idx]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In the shop, these MacBooks are encased in a soft rubber enclosure - so you will never know about the razor edge until you buy it, get it home, break the seal and use it (very clever con).',\n",
       " 'rubber enclosure',\n",
       " 50,\n",
       " 66,\n",
       " 'positive')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sample['text']\n",
    "aspects = sample['aspects']\n",
    "term, from_, to, polarity = aspects[0]['term'], aspects[0]['from'], aspects[0]['to'], aspects[0]['polarity']\n",
    "\n",
    "text, term, from_, to, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1130, 1103, 4130, 117, 1292, 6603, 2064, 9753, 1116, 1132, 4035, 14083, 1181, 1107, 170, 2991, 9579, 19904, 118, 1177, 1128, 1209, 1309, 1221, 1164, 1103, 20015, 2652, 1235, 1128, 4417, 1122, 117, 1243, 1122, 1313, 117, 2549, 1103, 9438, 1105, 1329, 1122, 113, 1304, 13336, 14255, 114, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 6), (7, 11), (11, 12), (13, 18), (19, 22), (22, 23), (23, 26), (26, 27), (28, 31), (32, 34), (34, 38), (38, 39), (40, 42), (43, 44), (45, 49), (50, 56), (57, 66), (67, 68), (69, 71), (72, 75), (76, 80), (81, 86), (87, 91), (92, 97), (98, 101), (102, 107), (108, 112), (113, 118), (119, 122), (123, 126), (127, 129), (129, 130), (131, 134), (135, 137), (138, 142), (142, 143), (144, 149), (150, 153), (154, 158), (159, 162), (163, 166), (167, 169), (170, 171), (171, 175), (176, 182), (183, 186), (186, 187), (187, 188), (0, 0)]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer(text, padding=True, max_length=100, truncation=True, return_offsets_mapping=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] In the shop, these MacBooks are encased in a soft rubber enclosure - so you will never know about the razor edge until you buy it, get it home, break the seal and use it ( very clever con ). [SEP]'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenized['input_ids']\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'In', 'the', 'shop', ',', 'these', 'Mac', '##B', '##ook', '##s', 'are', 'en', '##case', '##d', 'in', 'a', 'soft', 'rubber', 'enclosure', '-', 'so', 'you', 'will', 'never', 'know', 'about', 'the', 'razor', 'edge', 'until', 'you', 'buy', 'it', ',', 'get', 'it', 'home', ',', 'break', 'the', 'seal', 'and', 'use', 'it', '(', 'very', 'clever', 'con', ')', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(text, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(sample, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate tags for each token in the text\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    aspect_idx = 0\n",
    "    started = False\n",
    "    all_aspects_covered = False\n",
    "\n",
    "    text, aspects = sample['text'], sample['aspects']\n",
    "    aspects = sorted(aspects, key=lambda x: x['from'])\n",
    "\n",
    "    tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
    "    tokenized = tokenizer(text, padding=True, truncation=True, return_offsets_mapping=True)\n",
    "\n",
    "    for token, offset in zip(tokens, tokenized['offset_mapping']):\n",
    "\n",
    "        from_ = offset[0]; to = offset[1]\n",
    "        if from_ == to == 0:\n",
    "            tag = 'Q'  # represents the ignore tag for special tokens\n",
    "\n",
    "        elif all_aspects_covered or to < aspects[aspect_idx]['from']: # either all aspects are covered or the current token is before the current aspect\n",
    "            tag= 'O' # represents the tag for non-aspect tokens\n",
    "            started = False\n",
    "\n",
    "        elif from_ >= aspects[aspect_idx]['from'] and to <= aspects[aspect_idx]['to']: # If the current token boundaries are within the aspect boundaries\n",
    "            if not started:\n",
    "                started = True\n",
    "                tag = 'B'\n",
    "            else:\n",
    "                if token.startswith(\"##\"):\n",
    "                    tag = 'X' # represents the tag for subtokens\n",
    "                else:\n",
    "                    tag = 'I' # represents the tag for continuing aspect\n",
    "\n",
    "        if not all_aspects_covered and to >= aspects[aspect_idx]['to']: # If the current token boundaries are after the aspect boundaries\n",
    "            aspect_idx += 1 # move to the next aspect\n",
    "            if aspect_idx >= len(aspects):\n",
    "                all_aspects_covered = True\n",
    "            started = False\n",
    "\n",
    "        tags.append(tag)\n",
    "\n",
    "    return tags, tokens, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 'Q'), ('In', 'O'), ('the', 'O'), ('shop', 'O'), (',', 'O'), ('these', 'O'), ('Mac', 'O'), ('##B', 'O'), ('##ook', 'O'), ('##s', 'O'), ('are', 'O'), ('en', 'O'), ('##case', 'O'), ('##d', 'O'), ('in', 'O'), ('a', 'O'), ('soft', 'O'), ('rubber', 'B'), ('enclosure', 'I'), ('-', 'O'), ('so', 'O'), ('you', 'O'), ('will', 'O'), ('never', 'O'), ('know', 'O'), ('about', 'O'), ('the', 'O'), ('razor', 'O'), ('edge', 'B'), ('until', 'O'), ('you', 'O'), ('buy', 'O'), ('it', 'O'), (',', 'O'), ('get', 'O'), ('it', 'O'), ('home', 'O'), (',', 'O'), ('break', 'O'), ('the', 'O'), ('seal', 'O'), ('and', 'O'), ('use', 'O'), ('it', 'O'), ('(', 'O'), ('very', 'O'), ('clever', 'O'), ('con', 'O'), (')', 'O'), ('.', 'O'), ('[SEP]', 'Q')]\n"
     ]
    }
   ],
   "source": [
    "tags, tokens, tokenized = generate_tags(sample, tokenizer)\n",
    "print(list(zip(tokens, tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = {\n",
    "    \"text\": \"This is a nice lappy headphones camera set!!!\",\n",
    "    \"aspects\": [\n",
    "        {\"term\": \"lappy\", \"from\": 15, \"to\": 20, \"polarity\": \"positive\"},\n",
    "        {\"term\": \"headphones\", \"from\": 21, \"to\": 31, \"polarity\": \"positive\"},\n",
    "        {\"term\": \"camera set\", \"from\": 32, \"to\": 42, \"polarity\": \"positive\"}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Q\n",
      "This O\n",
      "is O\n",
      "a O\n",
      "nice O\n",
      "lap B\n",
      "##py X\n",
      "head B\n",
      "##phones X\n",
      "camera B\n",
      "set I\n",
      "! O\n",
      "! O\n",
      "! O\n",
      "[SEP] Q\n"
     ]
    }
   ],
   "source": [
    "tags, tokens, tokenized = generate_tags(new_sample, tokenizer)\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = {\n",
    "    \"text\": \"My name is Rishabh Gupta!\",\n",
    "    \"aspects\": [\n",
    "        {\"term\": \"Rishabh Gupta\", \"from\": 11, \"to\": 24, \"polarity\": \"positive\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Q\n",
      "My O\n",
      "name O\n",
      "is O\n",
      "R B\n",
      "##ish X\n",
      "##ab X\n",
      "##h X\n",
      "Gupta I\n",
      "! O\n",
      "[SEP] Q\n"
     ]
    }
   ],
   "source": [
    "tags, tokens, tokenized = generate_tags(new_sample, tokenizer)\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b91a4b4bd425fd304c2646209932bd24121a9132bf2c68c196c416ea21ab788"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
